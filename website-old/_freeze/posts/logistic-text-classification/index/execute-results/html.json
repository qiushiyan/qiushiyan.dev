{
  "hash": "f9a21add8ffd87386ca8b33c0c5927e6",
  "result": {
    "markdown": "---\ntitle: Text classification with penalized logistic regression\ndate: '2020-05-02'\ncategories:\n  - Machine Learning\n  - R\ndescription: 'Train a classification model with tidymodels to distinguish Charlotte Brontë from Emily Brontë'\nbibliography: ref.bib\nimage: featured.jpeg\n---\n\n\n\nThis article demonstrates a modeling example using the [`tidymodels`](https://www.tidymodels.org/) framework for text classification. Data are downloaded via the [`gutenbergr`](https://docs.ropensci.org/gutenbergr/) package, including 5 books written by either Emily Brontë or Charlotte Brontë. The goal is to predict the author given words in a line, that is the probability of line being written by one sister instead of another.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(gutenbergr)\n```\n:::\n\n\n\nThe cleaned `books` dataset contains linse as individual rows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmirror_url <- \"http://mirrors.xmission.com/gutenberg/\"\n\nbooks <- gutenberg_works(author %in% c(\"Brontë, Emily\", \"Brontë, Charlotte\")) %>%\n  gutenberg_download(meta_fields = c(\"title\", \"author\"), mirror = mirror_url) %>%\n  transmute(title,\n            author = if_else(author == \"Brontë, Emily\",\n                             \"Emily Brontë\",\n                             \"Charlotte Brontë\") %>% factor(),\n            line_index = row_number(),\n            text)\nbooks\n#> # A tibble: 89,588 × 4\n#>    title             author       line_index text               \n#>    <chr>             <fct>             <int> <chr>              \n#>  1 Wuthering Heights Emily Brontë          1 \"Wuthering Heights\"\n#>  2 Wuthering Heights Emily Brontë          2 \"\"                 \n#>  3 Wuthering Heights Emily Brontë          3 \"by Emily Brontë\"  \n#>  4 Wuthering Heights Emily Brontë          4 \"\"                 \n#>  5 Wuthering Heights Emily Brontë          5 \"\"                 \n#>  6 Wuthering Heights Emily Brontë          6 \"\"                 \n#>  7 Wuthering Heights Emily Brontë          7 \"\"                 \n#>  8 Wuthering Heights Emily Brontë          8 \"CHAPTER I\"        \n#>  9 Wuthering Heights Emily Brontë          9 \"\"                 \n#> 10 Wuthering Heights Emily Brontë         10 \"\"                 \n#> # … with 89,578 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nTo obtain tidy text structure illustrated in [Text Mining with R](https://www.tidytextmining.com/), I use `unnest_tokens()` to perform tokenization and remove all the stop words. I also removed characters like `'`, `'s`, `'` and whitespaces to return valid column names after widening. But it turns out this served as some sort of stemming too! (heathcliff's becomes heathcliff). Then low frequency words (whose frequency is less than 0.05% of an author's total word counts) are removed. The cutoff may be a little too high if you plot that histogram, but I really need this to save computation efforts on my laptop :sweat_smile:.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_books <- books %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words) %>%\n  filter(!str_detect(word, \"^\\\\d+$\")) %>%\n  mutate(word = str_remove_all(word, \"_|'s|'|\\\\s\"))\n\ntotal_words <- clean_books %>%\n  count(author, name = \"total\")\n\ntidy_books <- clean_books %>%\n  left_join(total_words) %>%\n  group_by(author, total, word) %>%\n  filter((n() / total) > 0.0005) %>%\n  ungroup()\n```\n:::\n\n\n# Comparing word frequency\n\nBefore building an actual predictive model, let's do some EDA to see different tendency to use a particular word! This will also shed light on what we would expect from the text classification. Now, we will compare word frequency (proportion) between the two sisters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books %>%\n  group_by(author, total) %>%\n  count(word) %>%\n  mutate(prop = n / total) %>%\n  ungroup() %>%\n  select(-total, -n) %>%\n  pivot_wider(names_from  = author, values_from = prop,\n              values_fill = list(prop = 0)) %>%\n  ggplot(aes(x = `Charlotte Brontë`, y = `Emily Brontë`,\n             color = abs(`Emily Brontë` -  `Charlotte Brontë`))) +\n  geom_jitter(width = 0.001, height = 0.001, alpha = 0.2, size = 2.5) +\n  geom_abline(color = \"gray40\", lty = 2) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, size = 7.5) +\n  scale_color_gradient(low = \"darkslategray4\", high = \"gray75\") +\n  scale_x_continuous(labels = scales::label_percent()) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  theme(legend.position = \"none\") +\n  coord_cartesian(xlim = c(0, NA)) +\n  theme(text = element_text(size = 18),\n        plot.title.position = \"plot\")\n```\n\n::: {.cell-output-display}\n![Comparing word frequency of two sisters](index_files/figure-html/fig-word-freq-1.png){#fig-word-freq width=1440}\n:::\n:::\n\n\n\nWords lie near the line such as \"home\", \"head\" and \"half\" indicate similar tendency to use that word, while those that are far from the line are words that are found more in one set of texts than another, for example \"headthcliff\", \"linton\", \"catherine\", etc.\n\nWhat does this plot tell us? Judged only by word frequency, it looks that there are a number of words that are quite characteristic of Emily Brontë (upper left corner). Charlotte, on the other hand, has few representative words (bottom right corner). We will investigate this further in the model.\n\n# Modeling\n\n## Data preprocessing\n\nThere are 428 and features (words) and 47266 observations in total. Approximately 18% of the response are 1 (Emily Brontë).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books %>%\n  count(author) %>%\n  mutate(prop = n / sum(n))\n#> # A tibble: 2 × 3\n#>   author               n  prop\n#>   <fct>            <int> <dbl>\n#> 1 Charlotte Brontë 64349 0.808\n#> 2 Emily Brontë     15310 0.192\n```\n:::\n\n\n\nNow it's time to widen our data to reach an appropriate model structure, this similar to a document-term matrix, with rows being a line and column word count.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nset.seed(2020)\ndoParallel::registerDoParallel()\n\nmodel_df <- tidy_books %>%\n  count(line_index, word) %>%\n  pivot_wider(names_from = word, values_from = n,\n              values_fill = list(n = 0)) %>%\n  left_join(books, by = c(\"line_index\" = \"line_index\")) %>%\n  select(-title, -text)\n\nmodel_df\n#> # A tibble: 47,266 × 430\n#>    line_index heights wuthering chapter returned visit heath…¹ black  eyes heart\n#>         <int>   <int>     <int>   <int>    <int> <int>   <int> <int> <int> <int>\n#>  1          1       1         1       0        0     0       0     0     0     0\n#>  2          8       0         0       1        0     0       0     0     0     0\n#>  3         11       0         0       0        1     1       0     0     0     0\n#>  4         15       0         0       0        0     0       1     0     0     0\n#>  5         17       0         0       0        0     0       0     1     1     1\n#>  6         19       0         0       0        0     0       0     0     0     0\n#>  7         22       0         0       0        0     0       1     0     0     0\n#>  8         24       0         0       0        0     0       0     0     0     0\n#>  9         26       0         0       0        0     0       0     0     0     0\n#> 10         27       0         0       0        0     0       0     0     0     0\n#> # … with 47,256 more rows, 420 more variables: fingers <int>, answer <int>,\n#> #   sir <int>, hope <int>, grange <int>, heard <int>, thrushcross <int>,\n#> #   interrupted <int>, walk <int>, closed <int>, uttered <int>, gate <int>,\n#> #   words <int>, hand <int>, entered <int>, joseph <int>, bring <int>,\n#> #   horse <int>, suppose <int>, nay <int>, dinner <int>, `heathcliff’s` <int>,\n#> #   times <int>, guess <int>, wind <int>, house <int>, set <int>, strong <int>,\n#> #   wall <int>, door <int>, earnshaw <int>, hareton <int>, short <int>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n```\n:::\n\n\n\n\n## Train a penalized logistic regression model\n\nSplit the data into training set and testing set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbook_split <- initial_split(model_df)\nbook_train <- training(book_split)\nbook_test <- testing(book_split)\n```\n:::\n\n\nSpecify a L1 penalized logistic model, center and scale all predictors and combine them in to a `workflow` object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_spec <- logistic_reg(penalty = 0.05, mixture = 1) %>%\n  set_engine(\"glmnet\")\n\nbook_rec <- recipe(author ~ ., data = book_train) %>%\n  update_role(line_index, new_role = \"ID\") %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\nbook_wf <- workflow() %>%\n  add_model(logistic_spec) %>%\n  add_recipe(book_rec)\n\ninitial_fit <- book_wf %>%\n  fit(data = book_train)\n```\n:::\n\n\n\n`initial_fit` is a simple fitted regression model without any hyperparameters. By default `glmnet` calls for 100 values of lambda even if I specify $\\lambda = 0.05$. So the extracted result aren't that helpful.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_fit %>%\n  extract_fit_parsnip() %>%\n  tidy()\n#> # A tibble: 429 × 3\n#>    term        estimate penalty\n#>    <chr>          <dbl>   <dbl>\n#>  1 (Intercept)   -1.63     0.05\n#>  2 heights        0        0.05\n#>  3 wuthering      0        0.05\n#>  4 chapter        0        0.05\n#>  5 returned       0        0.05\n#>  6 visit          0        0.05\n#>  7 heathcliff     0.134    0.05\n#>  8 black          0        0.05\n#>  9 eyes           0        0.05\n#> 10 heart          0        0.05\n#> # … with 419 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n```\n:::\n\n\nWe can make predictions with `initial_fit` anyway, and examine metrics like overall accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_predict <- predict(initial_fit, book_test) %>%\n    bind_cols(predict(initial_fit, book_test, type = \"prob\")) %>%\n    bind_cols(book_test %>% select(author, line_index))\n\ninitial_predict\n#> # A tibble: 11,817 × 5\n#>    .pred_class      `.pred_Charlotte Brontë` `.pred_Emily Brontë` author line_…¹\n#>    <fct>                               <dbl>                <dbl> <fct>    <int>\n#>  1 Charlotte Brontë                    0.840                0.160 Emily…      11\n#>  2 Charlotte Brontë                    0.557                0.443 Emily…      15\n#>  3 Charlotte Brontë                    0.840                0.160 Emily…      19\n#>  4 Charlotte Brontë                    0.840                0.160 Emily…      31\n#>  5 Charlotte Brontë                    0.840                0.160 Emily…      43\n#>  6 Charlotte Brontë                    0.840                0.160 Emily…      59\n#>  7 Charlotte Brontë                    0.840                0.160 Emily…      60\n#>  8 Charlotte Brontë                    0.840                0.160 Emily…      69\n#>  9 Charlotte Brontë                    0.840                0.160 Emily…      88\n#> 10 Charlotte Brontë                    0.840                0.160 Emily…      89\n#> # … with 11,807 more rows, and abbreviated variable name ¹​line_index\n#> # ℹ Use `print(n = ...)` to see more rows\n```\n:::\n\n\n\nHow good is our initial model?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_predict %>%\n  accuracy(truth = author, estimate = .pred_class)\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.836\n```\n:::\n\n\nNearly 84% of all predictions are right. This isn't a very statisfactory result since \"Charlotte Brontë\" accounts for 81% of `author`, making our model only slightly better than a classifier that would assngin all `author` with \"Charlotte Brontë\" anyway.\n\n### Tuning lambda\n\nWe can figure out an appropriate penalty using resampling and tune the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_wf_tune <- book_wf %>%\n  update_model(logistic_spec %>% set_args(penalty = tune()))\n\nlambda_grid <- grid_regular(penalty(), levels = 100)\nbook_folds <- vfold_cv(book_train, v = 10)\n```\n:::\n\n\nHere I build a set of 10 cross validations resamples, and set `levels = 100` to try 100 choices of lambda ranging from 0 to 1.\n\nThen I tune the grid:\n\n::: {.cell hash='index_cache/html/unnamed-chunk-14_5d66cbc6b295fdcc14cdce5d5c03f7e1'}\n\n```{.r .cell-code}\nlogistic_results <- logistic_wf_tune %>%\n  tune_grid(resamples = book_folds, grid = lambda_grid)\n```\n:::\n\n\nThere is an `autoplot()` method for the tuned results, but I instead plot two metrics versus lambda respectivcely by myself.\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_results %>%\n  collect_metrics() %>%\n  mutate(lower_bound = mean - std_err,\n         upper_bound = mean + std_err) %>%\n  ggplot(aes(penalty, mean)) +\n  geom_line(aes(color = .metric), size = 1.5, show.legend = FALSE) +\n  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound)) +\n  facet_wrap(~ .metric, nrow = 2, scales = \"free\") +\n  labs(y = NULL, x = expression(lambda))\n```\n\n::: {.cell-output-display}\n![Classificationm metrics across strength of L1 regularization](index_files/figure-html/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\nOk, the two metrics both display a monotone decrease as lambda increases, but does not exhibit much change once lambda is greater than 0.1, which is essentailly random guess according to the author's respective proportion of appearance in the data. This plot shows that the model is generally better at small penalty, suggesting that the majority of the predictors are fairly important to the model. We may lean towards larger penalty with slightly worse performance, bacause they lead to simpler models. It follows that we may want to choose lambda in top rows in the following data frame\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_models <- logistic_results %>%\n    show_best(\"roc_auc\", n = 100) %>%\n    arrange(desc(penalty)) %>%\n    filter(mean > 0.9)\n\ntop_models\n#> # A tibble: 77 × 7\n#>     penalty .metric .estimator  mean     n std_err .config               \n#>       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                 \n#>  1 0.00475  roc_auc binary     0.908    10 0.00188 Preprocessor1_Model077\n#>  2 0.00376  roc_auc binary     0.913    10 0.00194 Preprocessor1_Model076\n#>  3 0.00298  roc_auc binary     0.917    10 0.00210 Preprocessor1_Model075\n#>  4 0.00236  roc_auc binary     0.920    10 0.00183 Preprocessor1_Model074\n#>  5 0.00187  roc_auc binary     0.921    10 0.00202 Preprocessor1_Model073\n#>  6 0.00148  roc_auc binary     0.921    10 0.00221 Preprocessor1_Model072\n#>  7 0.00118  roc_auc binary     0.921    10 0.00222 Preprocessor1_Model071\n#>  8 0.000933 roc_auc binary     0.921    10 0.00210 Preprocessor1_Model070\n#>  9 0.000739 roc_auc binary     0.921    10 0.00211 Preprocessor1_Model069\n#> 10 0.000586 roc_auc binary     0.921    10 0.00206 Preprocessor1_Model068\n#> # … with 67 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n```\n:::\n\n\n`select_best()` with return the 9th row with $\\lambda \\approx 0.000586$ for its highest performance on `roc_auc`. But I'll stick to the parsimonious principle and pick $\\lambda \\approx 0.00376$ at the cost of a fall in `roc_auc` by 0.005 and in `accuracy` by 0.001.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_results %>%\n  select_best(metric = \"roc_auc\")\n#> # A tibble: 1 × 2\n#>    penalty .config               \n#>      <dbl> <chr>                 \n#> 1 0.000464 Preprocessor1_Model067\n\nbook_wf_final <- finalize_workflow(logistic_wf_tune,\n                                  parameters = top_models %>% slice(1))\n```\n:::\n\n\n\nNow the model specification in the workflow is filled with the picked lambda:\n\n::: {.cell}\n\n```{.r .cell-code}\nbook_wf_final %>% extract_spec_parsnip()\n#> Logistic Regression Model Specification (classification)\n#> \n#> Main Arguments:\n#>   penalty = 0.00475081016210279\n#>   mixture = 1\n#> \n#> Computational engine: glmnet\n```\n:::\n\n\nThe next thing is to fit the best model with the training set, and evaluate against the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_final <- last_fit(book_wf_final, split = book_split)\n\nlogistic_final %>%\n  collect_metrics()\n#> # A tibble: 2 × 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.938 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.910 Preprocessor1_Model1\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_final %>%\n  collect_predictions() %>%\n  roc_curve(truth = author, `.pred_Emily Brontë`) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=768}\n:::\n:::\n\n\n\nThe accuracy of our logisitc model rises by a rough 9% to 93.8%, with `roc_auc` being nearly 0.904. This is pretty good!\n\nThere is also the confusion matrix to check. The model does well in identifying Charlotte Brontë (low false positive rate, high sensitivity), yet suffers relatively high false negative rate (mistakenly identify 39% of Emily Brontë as Charlotte Brontë, aka low specificity). In part, this is due to class imbalance (four out of five books were written by Charlotte).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_final %>%\n  collect_predictions() %>%\n  conf_mat(truth = author, estimate = .pred_class)\n#>                   Truth\n#> Prediction         Charlotte Brontë Emily Brontë\n#>   Charlotte Brontë             9872          727\n#>   Emily Brontë                    1         1217\n```\n:::\n\n\n\nTo examine the effect of predictors, I agian use `fit` and `pull_workflow` to extract model fit. Variable importance plots implemented in the [vip](https://koalaverse.github.io/vip/index.html) package provides an intuitive way to visualize importance of predictors in this scenario, using the absolute value of the t-statistic as a measure of VI.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vip)\n\nlogistic_vi <- book_wf_final %>%\n  fit(book_train) %>%\n  extract_fit_parsnip() %>%\n  vi(lambda = top_models[1, ]$penalty) %>%\n  group_by(Sign) %>%\n  slice_max(order_by = abs(Importance), n = 30) %>%\n  ungroup() %>%\n  mutate(Sign = if_else(Sign == \"POS\",\n                        \"More Emily Brontë\",\n                        \"More Charlotte Brontë\"))\n\nlogistic_vi %>%\n  ggplot(aes(y = reorder_within(Variable, abs(Importance), Sign),\n             x = Importance)) +\n  geom_col(aes(fill = Sign),\n           show.legend = FALSE, alpha = 0.6) +\n  scale_y_reordered() +\n  facet_wrap(~ Sign, nrow = 1, scales = \"free\") +\n  labs(title = \"How word usage classifies Brontë sisters\",\n       x = NULL,\n       y = NULL) +\n  theme(axis.text = element_text(size = 18),\n        plot.title = element_text(size = 24),\n        plot.title.position = \"plot\")\n```\n\n::: {.cell-output-display}\n![Variable importance plot for penalized logistic regression](index_files/figure-html/unnamed-chunk-22-1.png){width=1056}\n:::\n:::\n\n\nIs it cheating to use names of a character to classify authors? Perhaps I should consider include more books and remove names for text classification next time.\n\nNote that variale importance in the left panel is generally smaller than the right, this corresponds to what we find in the word frequency plot @fig-word-freq that Emily Brontë has more and stronger characteristic words.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}